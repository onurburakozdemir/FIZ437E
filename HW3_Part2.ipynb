{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cMNIST_son.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onurburakozdemir/FIZ437E/blob/main/HW3_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Eru4DNVXit1-"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset2 = datasets.MNIST(root='./data', train=True, download=True)\n",
        "test_dataset2 = datasets.MNIST(root='./data', train=False)"
      ],
      "metadata": {
        "id": "n8SPHYd2ixOi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST_train_dataset = []\n",
        "MNIST_train_label = []\n",
        "for i, (img, label) in enumerate(train_dataset2):\n",
        "  img_arry = np.array(img)\n",
        "  dtype = img_arry.dtype\n",
        "  h, w = img_arry.shape\n",
        "  img_arry = np.reshape(img_arry, [h, w, 1])\n",
        "  img_arry = np.concatenate([img_arry, img_arry, img_arry], axis=2)\n",
        "  MNIST_train_dataset.append(img_arry)\n",
        "  MNIST_train_label.append(label)\n",
        "\n",
        "MNIST_test_dataset = []\n",
        "MNIST_test_label = []\n",
        "for i, (img, label) in enumerate(test_dataset2):\n",
        "  img_arry = np.array(img)\n",
        "  dtype = img_arry.dtype\n",
        "  h, w = img_arry.shape\n",
        "  img_arry = np.reshape(img_arry, [h, w, 1])\n",
        "  img_arry = np.concatenate([img_arry, img_arry, img_arry], axis=2)\n",
        "  MNIST_test_dataset.append(img_arry)\n",
        "  MNIST_test_label.append(label)"
      ],
      "metadata": {
        "id": "eyRskBJC7ApO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cMNIST_train_dataset = []\n",
        "cMNIST_train_label = []\n",
        "for i, (img, label) in enumerate(train_dataset2):\n",
        "  img_arry = np.array(img)\n",
        "  dtype = img_arry.dtype\n",
        "  h, w = img_arry.shape\n",
        "  img_arry = np.reshape(img_arry, [h, w, 1])\n",
        "  if i < 15000:\n",
        "    img_arry = np.concatenate([img_arry, \n",
        "                               np.zeros((h, w, 2), dtype=dtype)], axis=2)\n",
        "  elif i < 30000:\n",
        "    img_arry = np.concatenate([np.zeros((h, w, 2), dtype=dtype),\n",
        "                               img_arry], axis=2)\n",
        "  elif i < 45000:\n",
        "    img_arry = np.concatenate([np.zeros((h, w, 1), dtype=dtype), img_arry, \n",
        "                               np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
        "  else:\n",
        "    img_arry = np.concatenate([img_arry, img_arry, \n",
        "                          np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
        "  cMNIST_train_dataset.append(img_arry)\n",
        "  cMNIST_train_label.append(label)\n",
        "\n",
        "cMNIST_test_dataset = []\n",
        "cMNIST_test_label = []\n",
        "for i, (img, label) in enumerate(test_dataset2):\n",
        "  img_arry = np.array(img)\n",
        "  dtype = img_arry.dtype\n",
        "  h, w = img_arry.shape\n",
        "  img_arry = np.reshape(img_arry, [h, w, 1])\n",
        "  if i < 2500:\n",
        "    img_arry = np.concatenate([img_arry,\n",
        "                               np.zeros((h, w, 2), dtype=dtype)], axis=2)\n",
        "  elif i < 5000:\n",
        "    img_arry = np.concatenate([np.zeros((h, w, 2), dtype=dtype),\n",
        "                               img_arry], axis=2)\n",
        "  elif i < 7500:\n",
        "    img_arry = np.concatenate([np.zeros((h, w, 1), dtype=dtype), img_arry, \n",
        "                               np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
        "  else:\n",
        "    img_arry = np.concatenate([img_arry, img_arry, \n",
        "                          np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
        "  cMNIST_test_dataset.append(img_arry)\n",
        "  cMNIST_test_label.append(label)"
      ],
      "metadata": {
        "id": "bh6Qj-IGi-WM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_train = [(img.astype(np.float32), \n",
        "             cMNIST_train_label[i]) \n",
        "             for i, img in enumerate(cMNIST_train_dataset)]\n",
        "\n",
        "c_test = [(img.astype(np.float32), \n",
        "             cMNIST_test_label[i]) \n",
        "             for i, img in enumerate(cMNIST_test_dataset)]\n",
        "\n",
        "bw_train = [(img.astype(np.float32), \n",
        "             MNIST_train_label[i]) \n",
        "             for i, img in enumerate(MNIST_train_dataset)]\n",
        "\n",
        "bw_test = [(img.astype(np.float32), \n",
        "             MNIST_test_label[i]) \n",
        "             for i, img in enumerate(MNIST_test_dataset)]"
      ],
      "metadata": {
        "id": "a-MZEg22jAwK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_train_loader = DataLoader(c_train, batch_size=32)\n",
        "c_test_loader = DataLoader(c_test, batch_size=10000)\n",
        "bw_train_loader = DataLoader(bw_train, batch_size=32)\n",
        "bw_test_loader = DataLoader(bw_test, batch_size=10000)"
      ],
      "metadata": {
        "id": "myh73Nh1jFrE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(28*28*3, 64)\n",
        "        self.l2 = nn.Linear(64,64)\n",
        "        self.l3 = nn.Linear(64,10)\n",
        "        # dropout prevents overfitting of data\n",
        "        self.do = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1, 28 * 28 * 3)\n",
        "        # hidden layer, with relu activation function\n",
        "        h1 = nn.functional.relu(self.l1(x))\n",
        "        # hidden layer, with relu activation function\n",
        "        h2 =  nn.functional.relu(self.l2(h1))\n",
        "        # dropout layer\n",
        "        do = self.do(h2+h1)\n",
        "        # output layer\n",
        "        logits = self.l3(do)\n",
        "        return logits # initialize the NN"
      ],
      "metadata": {
        "id": "hNpPy2I2jRxH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "_v9S7wTdjK_a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal MNIST datasetinde eğitildi ve test edildi"
      ],
      "metadata": {
        "id": "o0rse4YY9m-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies = []\n",
        "train_loss = []\n",
        "test_accuracies = []\n",
        "test_loss = []\n",
        "n_epoch = 10\n",
        "for i in range(n_epoch):\n",
        "  accuracies=[]\n",
        "  losses=[]\n",
        "  print(i, end=' ')\n",
        "  for j, (images, labels) in enumerate(bw_train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images/255)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    accuracies.append(labels.eq(outputs.detach().argmax(dim=1)).float().mean())\n",
        "  train_accuracies.append(torch.tensor(accuracies).mean())\n",
        "  train_loss.append(torch.tensor(losses).mean())\n",
        "  print(torch.tensor(accuracies).mean(), end=' ')\n",
        "  print(torch.tensor(losses).mean(), end=' ')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for j, (images, labels) in enumerate(bw_test_loader):\n",
        "      outputs = model(images/255)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "    train_accuracies.append(labels.eq(outputs.detach().argmax(dim=1)).float().mean())\n",
        "    train_loss.append(loss.item())\n",
        "    print(labels.eq(outputs.detach().argmax(dim=1)).float().mean(), end=' ')\n",
        "    print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2auhi821jSzl",
        "outputId": "f72017f3-e737-4b03-afd8-26f85131e216"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.8425) tensor(0.6004) tensor(0.9029) 0.3337898552417755\n",
            "1 tensor(0.9144) tensor(0.3013) tensor(0.9254) 0.2559989094734192\n",
            "2 tensor(0.9305) tensor(0.2427) tensor(0.9347) 0.22160862386226654\n",
            "3 tensor(0.9410) tensor(0.2055) tensor(0.9426) 0.19433791935443878\n",
            "4 tensor(0.9480) tensor(0.1806) tensor(0.9486) 0.16881237924098969\n",
            "5 tensor(0.9536) tensor(0.1599) tensor(0.9531) 0.15341408550739288\n",
            "6 tensor(0.9591) tensor(0.1436) tensor(0.9573) 0.14373748004436493\n",
            "7 tensor(0.9622) tensor(0.1315) tensor(0.9594) 0.13502110540866852\n",
            "8 tensor(0.9651) tensor(0.1207) tensor(0.9623) 0.1256025731563568\n",
            "9 tensor(0.9669) tensor(0.1133) tensor(0.9622) 0.12460730969905853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Logistic_Reg_Model(28*28*3)\n",
        "model2 = ResNet()\n",
        "# model = LogisticRegression(28*28*3, 10)\n",
        "# model = MyNet(28*28*3, 100, 50, 10)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "11WkOu5z6wye"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CMNIST'de eğitildi ve test edildi"
      ],
      "metadata": {
        "id": "SauywSdJ-kyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies = []\n",
        "train_loss = []\n",
        "test_accuracies = []\n",
        "test_loss = []\n",
        "n_epoch = 10\n",
        "for i in range(n_epoch):\n",
        "  accuracies=[]\n",
        "  losses=[]\n",
        "  print(i, end=' ')\n",
        "  for j, (images, labels) in enumerate(c_train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model2(images/255)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    accuracies.append(labels.eq(outputs.detach().argmax(dim=1)).float().mean())\n",
        "  train_accuracies.append(torch.tensor(accuracies).mean())\n",
        "  train_loss.append(torch.tensor(losses).mean())\n",
        "  print(torch.tensor(accuracies).mean(), end=' ')\n",
        "  print(torch.tensor(losses).mean(), end=' ')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for j, (images, labels) in enumerate(c_test_loader):\n",
        "      outputs = model2(images/255)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "    train_accuracies.append(labels.eq(outputs.detach().argmax(dim=1)).float().mean())\n",
        "    train_loss.append(loss.item())\n",
        "    print(labels.eq(outputs.detach().argmax(dim=1)).float().mean(), end=' ')\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "id": "FvvXzx4WjXsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3aab84-89ff-4f70-99a1-f93b95550b08"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.0831) tensor(2.3127) tensor(0.0759) 2.315528154373169\n",
            "1 tensor(0.0829) tensor(2.3126) tensor(0.0781) 2.315495014190674\n",
            "2 tensor(0.0835) tensor(2.3127) tensor(0.0774) 2.315092086791992\n",
            "3 tensor(0.0834) tensor(2.3126) tensor(0.0803) 2.314739942550659\n",
            "4 tensor(0.0829) tensor(2.3127) tensor(0.0781) 2.3150243759155273\n",
            "5 tensor(0.0826) tensor(2.3126) tensor(0.0772) 2.3143301010131836\n",
            "6 tensor(0.0838) tensor(2.3128) tensor(0.0773) 2.3150463104248047\n",
            "7 tensor(0.0840) tensor(2.3126) tensor(0.0773) 2.3153791427612305\n",
            "8 tensor(0.0829) tensor(2.3127) tensor(0.0783) 2.3151803016662598\n",
            "9 tensor(0.0824) tensor(2.3127) tensor(0.0729) 2.3152458667755127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Logistic_Reg_Model(28*28*3)\n",
        "model3 = ResNet()\n",
        "# model = LogisticRegression(28*28*3, 10)\n",
        "# model = MyNet(28*28*3, 100, 50, 10)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "GOJuJOLr-CZw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CMNIST'de eğitildi ve Normal MNIST'de test edildi"
      ],
      "metadata": {
        "id": "XQbXwkL7--cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies = []\n",
        "train_loss = []\n",
        "test_accuracies = []\n",
        "test_loss = []\n",
        "n_epoch = 10\n",
        "for i in range(n_epoch):\n",
        "  accuracies=[]\n",
        "  losses=[]\n",
        "  print(i, end=' ')\n",
        "  for j, (images, labels) in enumerate(c_train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model3(images/255)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    accuracies.append(labels.eq(outputs.detach().argmax(dim=1)).float().mean())\n",
        "  train_accuracies.append(torch.tensor(accuracies).mean())\n",
        "  train_loss.append(torch.tensor(losses).mean())\n",
        "  print(torch.tensor(accuracies).mean(), end=' ')\n",
        "  print(torch.tensor(losses).mean(), end=' ')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for j, (images, labels) in enumerate(bw_test_loader):\n",
        "      outputs = model3(images/255)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "    train_accuracies.append(labels.eq(outputs.detach().argmax(dim=1)).float().mean())\n",
        "    train_loss.append(loss.item())\n",
        "    print(labels.eq(outputs.detach().argmax(dim=1)).float().mean(), end=' ')\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4oK5PMX-DGz",
        "outputId": "cdaf49ad-5ff5-424f-e028-38458edcde82"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.0926) tensor(2.3046) tensor(0.0905) 2.2959554195404053\n",
            "1 tensor(0.0925) tensor(2.3049) tensor(0.0916) 2.2955777645111084\n",
            "2 tensor(0.0935) tensor(2.3047) tensor(0.0902) 2.2963926792144775\n",
            "3 tensor(0.0934) tensor(2.3048) tensor(0.0914) 2.2966854572296143\n",
            "4 tensor(0.0923) tensor(2.3046) tensor(0.0885) 2.2959213256835938\n",
            "5 tensor(0.0934) tensor(2.3046) tensor(0.0913) 2.296086311340332\n",
            "6 tensor(0.0926) tensor(2.3046) tensor(0.0906) 2.2961907386779785\n",
            "7 tensor(0.0928) tensor(2.3047) tensor(0.0931) 2.2961854934692383\n",
            "8 tensor(0.0940) tensor(2.3046) tensor(0.0926) 2.296344041824341\n",
            "9 tensor(0.0931) tensor(2.3047) tensor(0.0930) 2.2959961891174316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Logistic_Reg_Model(28*28*3)\n",
        "model4 = ResNet()\n",
        "# model = LogisticRegression(28*28*3, 10)\n",
        "# model = MyNet(28*28*3, 100, 50, 10)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "v1_ThjX4_KaD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies = []\n",
        "train_loss = []\n",
        "test_accuracies = []\n",
        "test_loss = []\n",
        "n_epoch = 10\n",
        "for i in range(n_epoch):\n",
        "  accuracies=[]\n",
        "  losses=[]\n",
        "  print(i, end=' ')\n",
        "  for j, (images, labels) in enumerate(bw_train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model4(images/255)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    accuracies.append(labels.eq(outputs.detach().argmax(dim=1)).float().mean())\n",
        "  train_accuracies.append(torch.tensor(accuracies).mean())\n",
        "  train_loss.append(torch.tensor(losses).mean())\n",
        "  print(torch.tensor(accuracies).mean(), end=' ')\n",
        "  print(torch.tensor(losses).mean(), end=' ')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for j, (images, labels) in enumerate(c_test_loader):\n",
        "      outputs = model4(images/255)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "    train_accuracies.append(labels.eq(outputs.detach().argmax(dim=1)).float().mean())\n",
        "    train_loss.append(loss.item())\n",
        "    print(labels.eq(outputs.detach().argmax(dim=1)).float().mean(), end=' ')\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-roYm36_NZ0",
        "outputId": "f8142ef4-4802-4d35-9c43-903ad9759eed"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(0.1045) tensor(2.3142) tensor(0.1102) 2.3083343505859375\n",
            "1 tensor(0.1045) tensor(2.3141) tensor(0.1134) 2.308250665664673\n",
            "2 tensor(0.1050) tensor(2.3144) tensor(0.1099) 2.3090243339538574\n",
            "3 tensor(0.1043) tensor(2.3143) tensor(0.1112) 2.308685541152954\n",
            "4 tensor(0.1045) tensor(2.3143) tensor(0.1090) 2.3083908557891846\n",
            "5 tensor(0.1045) tensor(2.3141) tensor(0.1094) 2.308593988418579\n",
            "6 tensor(0.1049) tensor(2.3144) tensor(0.1132) 2.308384656906128\n",
            "7 tensor(0.1037) tensor(2.3144) tensor(0.1089) 2.3083813190460205\n",
            "8 tensor(0.1045) tensor(2.3142) tensor(0.1109) 2.30826473236084\n",
            "9 tensor(0.1047) tensor(2.3143) tensor(0.1115) 2.308321237564087\n"
          ]
        }
      ]
    }
  ]
}